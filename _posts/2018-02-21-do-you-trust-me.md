---
layout: "post"
title: "Do you trust me?"
date: "2018-02-21"
---
Most of what we learn in medicine is not because we have scrutinized the evidence. We learn from resources that have been deemed trustworthy, such as teachers, textbooks, lecturers, but we are not explicitly taught how to evaluate the evidence ourselves regardless of the resource. The more cohesive their story the easier it is to believe it and the more confidence we have on the resources and the story itself.

In Thinking, Fast and Slow Daniel Kahneman states:

>System 1 is designed to jump to conclusions from little evidence—and it is not designed to know the size of its jumps. Because of WYSIATI [What You See Is All There Is], only the evidence at hand counts. Because of confidence by coherence, the subjective confidence we have in our opinions reflects the coherence of the story that System 1 and System 2 have constructed. The amount of evidence and its quality do not count for much, because poor evidence can make a very good story. For some of our most important beliefs we have no evidence at all, except that people we love and trust hold these beliefs. Considering how little we know, the confidence we have in our beliefs is preposterous—and it is also essential.[^1]

Due to our associative machinery and the lazy System 2 we rather not put in the cognitive effort that it takes into understanding how science works. We tend to confuse our understand of difficult concepts in science with how much we like the group of people we belong to and their choice of resources. But the history and philosophy of science teach us that scientific knowledge is not dependent on the strength of a bond a group of people feel about each other. This faulty notion and many current research and educational practices do not align with learning critical thinking and adopting a scientific attitude.

One of the most difficult problems encountered in research is [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) a practice some might even consider to be pseudoscientific. This practice seems to be embedded in academic institutions. According to Geoff Norman,

>We are guilty of institutionalizing confirmation bias—every study we conduct, regardless of the findings, can be interpreted either as supporting the validity of the measure, or non-contributory. As a result, we don’t accumulate sufficient evidence to “falsify the hypothesis” and to claim that the instrument is not valid and should be abandoned.[^2]

Under confirmation bias disconfirming evidence is ignored while cherry picked data is selected. This practice aligns well with how our minds prefers a coherent story while ignoring other relevant and/or contradicting data. Kahneman reminds us that the world makes less sense than we tend to think, but we rather have the cognitive ease from a coherent story with less information than put in the cognitive effort in trying to understand how uncertain the world actually is. Science, on the other hand, helps us with understanding how the world works, but it requires transparency and examination of all kinds of evidence.

In 2008 Colliver and McGaghie wrote an article calling into question the reputation of medical education research and the lack of reporting of evidence of threat to validity. They stated,

>If a threat [to validity] cannot be ruled out, the researcher must capitulate and conclude that a research effect may be due to a confound, that research findings do not establish the effect of an intervention, or—even harder—that nothing can be concluded.[^3]

This is quite relevant in the state we are in. We are currently facing a [replication crisis](https://en.wikipedia.org/wiki/Replication_crisis) in science due to various causes. Some problems have to do with misunderstanding and misapplication of statistical procedures.[^4],[^5],[^6] Also, the [publish or perish](https://en.wikipedia.org/wiki/Publish_or_perish) practice in academia has lead to a miscontrued scientific attitude which has an "[unhealthy obsession with p values](https://www.vox.com/2016/3/15/11225162/p-value-simple-definition-hacking)." The tendency of reporting only statistically significant research while not publishing non-statistically significant findings is known as a form of [publication bias](https://en.wikipedia.org/wiki/Publication_bias) and it may be due to various practices. In a recent systematic appraisal in the health professions education literature Abbott et al[^7] report,

> We found reporting of P values and confidence intervals in HPER publications increased from the 1970s to 2015, and in 2015, P values were reported in most HPER abstracts and main texts of published research papers. However, reporting of confidence intervals and power analyses remained uncommon and lagged behind reporting in general biomedical research. In addition, most reported P values were statistically significant according to the standard threshold of P ≤ .05, which seems likely to reflect selective (biased) reporting.[^8]

It is unarguable that confirmation bias and publication bias complement each other and distort how science is supposed to work. These problems are not unique to medical education research which may also suffer from its own replication crisis[^9] and questionable research practices.[^10],[^11] It also doesn't help that our minds prefer to believe coherent stories from people we trust, even when these stories lack information. It is ironic that medical education talks a lot about entrustment, but it actually does not teach critical thinking and how science works. Science is not a perfect endeavour and as a social practice has its own pit falls. Susan Haack reminds us,

>as with every human enterprise, [science's] integrity can be threatened by political pressure, by commercial interests, or by simple vanity, greed, or pride.[^12]



#### Reference:

[^1]: Kahneman, D., Thinking, Fast and Slow., 2011

[^2]: Norman, G., Is psychometrics science?, Adv Health Sci Educ Theory Pract. 2016 Oct;21(4):731-4. doi: 10.1007/s10459-016-9705-6.

[^3]: Colliver J., McGaghie W., The reputation of medical education research: quasi-experimentation and unresolved threats to validity., Teach Learn Med. 2008 Apr-Jun;20(2):101-3. doi: 10.1080/10401330801989497.

[^4]: Greenland, S., et al., Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations., Eur J Epidemiol (2016) 31: 337. doi: 10.1007/s10654-016-0149-3

[^5]: Cohen, J. (1994). The earth is round (p < .05). American Psychologist, 49, 997-1003.

[^6]: Gigerenzer, G., Mindless statistics., The Journal of Socio-Economics
Volume 33, Issue 5, November 2004, Pages 587-606

[^7]: Abbott, E., (2017) Trends in P Value, Confidence Interval, and Power Analysis Reporting in Health Professions Education Research Reports: A Systematic Appraisal. Academic Medicine. Academic Medicine: February 2018 - Volume 93 - Issue 2 - p 314–323, DOI: 10.1097/ACM.0000000000001773

[^8]: Abbot, E., et al., "Tips For Reporting P Values, Confidence Intervals, And Power Analyses In Health Professions Education Research: Just Do It! AM Rounds." AM Rounds. N. p., 2017. Web. 21 Feb. 2018.

[^9]: Artino Jr., A. R. (2013), Why don't we conduct replication studies in medical education?. Med Educ, 47: 746–747. doi:10.1111/medu.12204

[^10]: Maggio, L. A., Artino, A. R., Picho, K., & Driessen, E. W. (2017). Are you sure you want to do that? Fostering the responsible conduct of medical education research. Academic Medicine: July 3, 2017, doi: 10.1097/ACM.0000000000001805

[^11]: Katherine Picho and Anthony R. Artino Jr (2016) 7 Deadly Sins in Educational Research. Journal of Graduate Medical Education: October 2016, Vol. 8, No. 4, pp. 483-487.

[^12]: Haack, S., Defending Science - within reason: Between Scientism and Cynicism., 2007
